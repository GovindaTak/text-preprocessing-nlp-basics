{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab71cd77-71c8-4d2a-bc39-ea887a05f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132b9c2c-3826-4de0-99b6-8fcdd157b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\govinda.tak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\govinda.tak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\govinda.tak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\govinda.tak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2167a4-c65a-47c5-888f-e0db2f980228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a916f5df-1f77-4814-bdeb-a40ea4519a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=\"NLTK (Natural Language Toolkit) is a popular Python library used for working with human language data (text) in the field of natural language processing (NLP). It provides tools for text processing, including tokenization, stemming, lemmatization, part-of-speech tagging, and more.\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5148b985-cdec-4362-b6c9-3689925093fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=\"To make the ExportDashboard component render on the right-hand side of the screen and align it horizontally with the heading, we can use CSS flexbox for layout management. Here's how you can achieve it:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb0432c-67cf-4915-9a35-a8c120ecaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=\"The SettingWithCopyWarning occurs because you are trying to modify a slice (a subset of a DataFrame), and Pandas cannot determine if you are working on the original DataFrame or a copy. This warning is common when you use chained operations or modify slices of data.\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1516621-a127-4b00-b43f-a2525d13da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[doc1,doc2,doc3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cb1db-cb17-49a3-b318-61701b44b77c",
   "metadata": {},
   "source": [
    "# Step 1 :-  Let's start tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ca57c56-d40a-4deb-b9e4-e54a3013fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'is', 'a', 'popular', 'Python', 'library', 'used', 'for', 'working', 'with', 'human', 'language', 'data', '(', 'text', ')', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'It', 'provides', 'tools', 'for', 'text', 'processing', ',', 'including', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'and', 'more', '.']\n",
      "doc 2 :-['To', 'make', 'the', 'ExportDashboard', 'component', 'render', 'on', 'the', 'right-hand', 'side', 'of', 'the', 'screen', 'and', 'align', 'it', 'horizontally', 'with', 'the', 'heading', ',', 'we', 'can', 'use', 'CSS', 'flexbox', 'for', 'layout', 'management', '.', 'Here', \"'s\", 'how', 'you', 'can', 'achieve', 'it', ':']\n",
      "doc 3 :-['The', 'SettingWithCopyWarning', 'occurs', 'because', 'you', 'are', 'trying', 'to', 'modify', 'a', 'slice', '(', 'a', 'subset', 'of', 'a', 'DataFrame', ')', ',', 'and', 'Pandas', 'can', 'not', 'determine', 'if', 'you', 'are', 'working', 'on', 'the', 'original', 'DataFrame', 'or', 'a', 'copy', '.', 'This', 'warning', 'is', 'common', 'when', 'you', 'use', 'chained', 'operations', 'or', 'modify', 'slices', 'of', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenize_corpus=[]\n",
    "for doc in corpus:\n",
    "    tokens=word_tokenize(doc)\n",
    "    tokenize_corpus.append(tokens);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(tokenize_corpus):\n",
    "    print(f'doc {i+1} :-{doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82819368-3a26-4886-8112-c59a45248f1c",
   "metadata": {},
   "source": [
    "# Step 2 :- lowercasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5686f729-a4e2-41f7-9f58-0c0cc0812bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['nltk', '(', 'natural', 'language', 'toolkit', ')', 'is', 'a', 'popular', 'python', 'library', 'used', 'for', 'working', 'with', 'human', 'language', 'data', '(', 'text', ')', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', '(', 'nlp', ')', '.', 'it', 'provides', 'tools', 'for', 'text', 'processing', ',', 'including', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'and', 'more', '.'] \n",
      " length :- 52\n",
      "doc 2 :-['to', 'make', 'the', 'exportdashboard', 'component', 'render', 'on', 'the', 'right-hand', 'side', 'of', 'the', 'screen', 'and', 'align', 'it', 'horizontally', 'with', 'the', 'heading', ',', 'we', 'can', 'use', 'css', 'flexbox', 'for', 'layout', 'management', '.', 'here', \"'s\", 'how', 'you', 'can', 'achieve', 'it', ':'] \n",
      " length :- 38\n",
      "doc 3 :-['the', 'settingwithcopywarning', 'occurs', 'because', 'you', 'are', 'trying', 'to', 'modify', 'a', 'slice', '(', 'a', 'subset', 'of', 'a', 'dataframe', ')', ',', 'and', 'pandas', 'can', 'not', 'determine', 'if', 'you', 'are', 'working', 'on', 'the', 'original', 'dataframe', 'or', 'a', 'copy', '.', 'this', 'warning', 'is', 'common', 'when', 'you', 'use', 'chained', 'operations', 'or', 'modify', 'slices', 'of', 'data', '.'] \n",
      " length :- 51\n"
     ]
    }
   ],
   "source": [
    "lowerCase_corpus=[]\n",
    "for doc in tokenize_corpus:\n",
    "    lowerCasing=[token.lower() for token in doc]\n",
    "    lowerCase_corpus.append(lowerCasing);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(lowerCase_corpus):\n",
    "    print(f'doc {i+1} :-{doc} \\n length :- {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c208d8-3c4a-4557-91a6-3a7ee93b8164",
   "metadata": {},
   "source": [
    "# Step 3:- removal of punctuation and special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4684267-b07e-4512-86c9-fadcf904ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['nltk', 'natural', 'language', 'toolkit', 'is', 'a', 'popular', 'python', 'library', 'used', 'for', 'working', 'with', 'human', 'language', 'data', 'text', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', 'nlp', 'it', 'provides', 'tools', 'for', 'text', 'processing', 'including', 'tokenization', 'stemming', 'lemmatization', 'tagging', 'and', 'more'] \n",
      " length :- 38\n",
      "doc 2 :-['to', 'make', 'the', 'exportdashboard', 'component', 'render', 'on', 'the', 'side', 'of', 'the', 'screen', 'and', 'align', 'it', 'horizontally', 'with', 'the', 'heading', 'we', 'can', 'use', 'css', 'flexbox', 'for', 'layout', 'management', 'here', 'how', 'you', 'can', 'achieve', 'it'] \n",
      " length :- 33\n",
      "doc 3 :-['the', 'settingwithcopywarning', 'occurs', 'because', 'you', 'are', 'trying', 'to', 'modify', 'a', 'slice', 'a', 'subset', 'of', 'a', 'dataframe', 'and', 'pandas', 'can', 'not', 'determine', 'if', 'you', 'are', 'working', 'on', 'the', 'original', 'dataframe', 'or', 'a', 'copy', 'this', 'warning', 'is', 'common', 'when', 'you', 'use', 'chained', 'operations', 'or', 'modify', 'slices', 'of', 'data'] \n",
      " length :- 46\n"
     ]
    }
   ],
   "source": [
    "no_pun_specChar_corpus=[]\n",
    "for doc in lowerCase_corpus:\n",
    "    remaing=[token for token in doc if token.isalpha()]\n",
    "    no_pun_specChar_corpus.append(remaing);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(no_pun_specChar_corpus):\n",
    "    print(f'doc {i+1} :-{doc} \\n length :- {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b22576-2833-4f46-adfc-b7684c30740b",
   "metadata": {},
   "source": [
    "# Step 4 :- removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d622d90a-70b8-4442-a169-e83ff0cc35c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['nltk', 'natural', 'language', 'toolkit', 'popular', 'python', 'library', 'used', 'working', 'human', 'language', 'data', 'text', 'field', 'natural', 'language', 'processing', 'nlp', 'provides', 'tools', 'text', 'processing', 'including', 'tokenization', 'stemming', 'lemmatization', 'tagging'] \n",
      " length :- 27\n",
      "doc 2 :-['make', 'exportdashboard', 'component', 'render', 'side', 'screen', 'align', 'horizontally', 'heading', 'use', 'css', 'flexbox', 'layout', 'management', 'achieve'] \n",
      " length :- 15\n",
      "doc 3 :-['settingwithcopywarning', 'occurs', 'trying', 'modify', 'slice', 'subset', 'dataframe', 'pandas', 'determine', 'working', 'original', 'dataframe', 'copy', 'warning', 'common', 'use', 'chained', 'operations', 'modify', 'slices', 'data'] \n",
      " length :- 21\n"
     ]
    }
   ],
   "source": [
    "no_stop_corpus=[]\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words.add('also')\n",
    "for doc in no_pun_specChar_corpus:\n",
    "    remaining=[token for token in doc if token not in stop_words]\n",
    "    no_stop_corpus.append(remaining);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(no_stop_corpus):\n",
    "    print(f'doc {i+1} :-{doc} \\n length :- {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86411d7a-7fda-42e2-9b5f-5ac336f78567",
   "metadata": {},
   "source": [
    "# Step 5 :- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85576b37-e0b2-467c-b666-98a49f55f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['nltk', 'natural', 'language', 'toolkit', 'popular', 'python', 'library', 'used', 'working', 'human', 'language', 'data', 'text', 'field', 'natural', 'language', 'processing', 'nlp', 'provides', 'tool', 'text', 'processing', 'including', 'tokenization', 'stemming', 'lemmatization', 'tagging'] \n",
      " length :- 27\n",
      "doc 2 :-['make', 'exportdashboard', 'component', 'render', 'side', 'screen', 'align', 'horizontally', 'heading', 'use', 'cs', 'flexbox', 'layout', 'management', 'achieve'] \n",
      " length :- 15\n",
      "doc 3 :-['settingwithcopywarning', 'occurs', 'trying', 'modify', 'slice', 'subset', 'dataframe', 'panda', 'determine', 'working', 'original', 'dataframe', 'copy', 'warning', 'common', 'use', 'chained', 'operation', 'modify', 'slice', 'data'] \n",
      " length :- 21\n"
     ]
    }
   ],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "lemmatize_corpus=[]\n",
    "for doc in no_stop_corpus:\n",
    "    remaning=[lem.lemmatize(token) for token in doc]\n",
    "    lemmatize_corpus.append(remaning);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(lemmatize_corpus):\n",
    "    print(f'doc {i+1} :-{doc} \\n length :- {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ea935-430a-4429-ac6b-95d0fb5d9a12",
   "metadata": {},
   "source": [
    "# Note :-\n",
    "so we can see lammatizer never reduce words to rooot form because it only reduce noun words. so we use Spacy library here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "803040dc-4391-46d1-be13-7a54f60e48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 :-['nltk', 'natural', 'language', 'toolkit', 'popular', 'python', 'library', 'use', 'work', 'human', 'language', 'datum', 'text', 'field', 'natural', 'language', 'processing', 'nlp', 'provide', 'tool', 'text', 'processing', 'include', 'tokenization', 'stem', 'lemmatization', 'tagging'] \n",
      " length :- 27\n",
      "doc 2 :-['make', 'exportdashboard', 'component', 'render', 'side', 'screen', 'align', 'horizontally', 'head', 'use', 'css', 'flexbox', 'layout', 'management', 'achieve'] \n",
      " length :- 15\n",
      "doc 3 :-['settingwithcopywarne', 'occur', 'try', 'modify', 'slice', 'subset', 'dataframe', 'panda', 'determine', 'work', 'original', 'dataframe', 'copy', 'warn', 'common', 'use', 'chain', 'operation', 'modify', 'slice', 'datum'] \n",
      " length :- 21\n"
     ]
    }
   ],
   "source": [
    "spacy_corpus=[]\n",
    "translator=spacy.load('en_core_web_sm')\n",
    "for doc in no_stop_corpus:\n",
    "    convertDoc=translator(' '.join(doc))\n",
    "    spacyDoc=[token.lemma_ for token in convertDoc]\n",
    "    spacy_corpus.append(spacyDoc);\n",
    "\n",
    "#print:-\n",
    "for i,doc in enumerate(spacy_corpus):\n",
    "    print(f'doc {i+1} :-{doc} \\n length :- {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e0ceb-9c86-4354-9e70-a5f0ee7f80a1",
   "metadata": {},
   "source": [
    "# Step 6 :- Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "effbc3d2-2112-45dc-85ce-63c52ef9a320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achieve', 'align', 'chain', 'common', 'component', 'copy', 'css', 'dataframe', 'datum', 'determine', 'exportdashboard', 'field', 'flexbox', 'head', 'horizontally', 'human', 'include', 'language', 'layout', 'lemmatization', 'library', 'make', 'management', 'modify', 'natural', 'nlp', 'nltk', 'occur', 'operation', 'original', 'panda', 'popular', 'processing', 'provide', 'python', 'render', 'screen', 'settingwithcopywarne', 'side', 'slice', 'stem', 'subset', 'tagging', 'text', 'tokenization', 'tool', 'toolkit', 'try', 'use', 'warn', 'work']\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "vocabulary=set()\n",
    "for doc in spacy_corpus:\n",
    "    for token in doc:\n",
    "        vocabulary.add(token)\n",
    "\n",
    "vocabulary=sorted(list(vocabulary))\n",
    "print(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6513119-15da-41e4-ae4f-e4ccdb6b4c24",
   "metadata": {},
   "source": [
    "# Step 7:- Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff14cae-96bf-4ce9-a95b-760b9e3bb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cc4f430-2a8f-4b8e-ad9a-9c824c255908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features names :-  ['achieve' 'align' 'chain' 'common' 'component' 'copy' 'css' 'dataframe'\n",
      " 'datum' 'determine' 'exportdashboard' 'field' 'flexbox' 'head'\n",
      " 'horizontally' 'human' 'include' 'language' 'layout' 'lemmatization'\n",
      " 'library' 'make' 'management' 'modify' 'natural' 'nlp' 'nltk' 'occur'\n",
      " 'operation' 'original' 'panda' 'popular' 'processing' 'provide' 'python'\n",
      " 'render' 'screen' 'settingwithcopywarne' 'side' 'slice' 'stem' 'subset'\n",
      " 'tagging' 'text' 'tokenization' 'tool' 'toolkit' 'try' 'use' 'warn'\n",
      " 'work']\n",
      "Dcoument-Term matrix shape :  (3, 51)\n",
      "frequency counts : \n",
      " [[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0\n",
      "  0 0 0 0 1 0 1 1 1 1 1 0 1 0 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 1 0 1 0 1 0 0 0 0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vector=CountVectorizer(vocabulary=vocabulary,binary=True) #it will encode document , so we need to form document.\n",
    "documents=[\" \".join(doc) for doc in spacy_corpus]\n",
    "\n",
    "x=vector.fit_transform(documents)\n",
    "features_name=vector.get_feature_names_out()\n",
    "\n",
    "print(\"features names :- \",features_name)\n",
    "print(\"Dcoument-Term matrix shape : \",x.shape)\n",
    "\n",
    "#to see frequency count\n",
    "frequency_count=x.toarray()\n",
    "print(\"frequency counts : \\n\",frequency_count)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6efd89dc-40fc-4557-a008-f9f697886f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>achieve</th>\n",
       "      <th>align</th>\n",
       "      <th>chain</th>\n",
       "      <th>common</th>\n",
       "      <th>component</th>\n",
       "      <th>copy</th>\n",
       "      <th>css</th>\n",
       "      <th>dataframe</th>\n",
       "      <th>datum</th>\n",
       "      <th>determine</th>\n",
       "      <th>...</th>\n",
       "      <th>subset</th>\n",
       "      <th>tagging</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>tool</th>\n",
       "      <th>toolkit</th>\n",
       "      <th>try</th>\n",
       "      <th>use</th>\n",
       "      <th>warn</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  achieve align chain common component copy css dataframe datum determine  \\\n",
       "0       0     0     0      0         0    0   0         0     1         0   \n",
       "1       1     1     0      0         1    0   1         0     0         0   \n",
       "2       0     0     1      1         0    1   0         1     1         1   \n",
       "\n",
       "   ... subset tagging text tokenization tool toolkit try use warn work  \n",
       "0  ...      0       1    1            1    1       1   0   1    0    1  \n",
       "1  ...      0       0    0            0    0       0   0   1    0    0  \n",
       "2  ...      1       0    0            0    0       0   1   1    1    1  \n",
       "\n",
       "[3 rows x 51 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x.toarray(),columns=[features_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5d7f7-f4c0-40c1-b9c5-035d0036624a",
   "metadata": {},
   "source": [
    " Note :- above is document matrix where the word is present or not in document is showing by 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e040877-a2f8-4a78-88ca-6efd6f463629",
   "metadata": {},
   "source": [
    "# B) Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "29595ede-c4c8-4480-b8c0-a03ffa3469cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features names :-  ['achieve' 'align' 'chain' 'common' 'component' 'copy' 'css' 'dataframe'\n",
      " 'datum' 'determine' 'exportdashboard' 'field' 'flexbox' 'head'\n",
      " 'horizontally' 'human' 'include' 'language' 'layout' 'lemmatization'\n",
      " 'library' 'make' 'management' 'modify' 'natural' 'nlp' 'nltk' 'occur'\n",
      " 'operation' 'original' 'panda' 'popular' 'processing' 'provide' 'python'\n",
      " 'render' 'screen' 'settingwithcopywarne' 'side' 'slice' 'stem' 'subset'\n",
      " 'tagging' 'text' 'tokenization' 'tool' 'toolkit' 'try' 'use' 'warn'\n",
      " 'work']\n",
      "Dcoument-Term matrix shape :  (3, 51)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "frequency counts : \n",
      " [[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 3 0 1 1 0 0 0 2 1 1 0 0 0 0 1 2 1 1 0\n",
      "  0 0 0 0 1 0 1 2 1 1 1 0 1 0 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 1 0 1 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 1 0 2 0 1 0 0 0 0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vector=CountVectorizer(vocabulary=vocabulary) #it will encode document , so we need to form document.\n",
    "documents=[\" \".join(doc) for doc in spacy_corpus]\n",
    "\n",
    "x=vector.fit_transform(documents)\n",
    "features_name=vector.get_feature_names_out()\n",
    "\n",
    "print(\"features names :- \",features_name)\n",
    "print(\"Dcoument-Term matrix shape : \",x.shape)\n",
    "print(type(x))\n",
    "#to see frequency count\n",
    "frequency_count=x.toarray()\n",
    "print(\"frequency counts : \\n\",frequency_count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c132328f-58e4-4efa-a8b0-c39ad0395e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>achieve</th>\n",
       "      <th>align</th>\n",
       "      <th>chain</th>\n",
       "      <th>common</th>\n",
       "      <th>component</th>\n",
       "      <th>copy</th>\n",
       "      <th>css</th>\n",
       "      <th>dataframe</th>\n",
       "      <th>datum</th>\n",
       "      <th>determine</th>\n",
       "      <th>...</th>\n",
       "      <th>subset</th>\n",
       "      <th>tagging</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>tool</th>\n",
       "      <th>toolkit</th>\n",
       "      <th>try</th>\n",
       "      <th>use</th>\n",
       "      <th>warn</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  achieve align chain common component copy css dataframe datum determine  \\\n",
       "0       0     0     0      0         0    0   0         0     1         0   \n",
       "1       1     1     0      0         1    0   1         0     0         0   \n",
       "2       0     0     1      1         0    1   0         2     1         1   \n",
       "\n",
       "   ... subset tagging text tokenization tool toolkit try use warn work  \n",
       "0  ...      0       1    2            1    1       1   0   1    0    1  \n",
       "1  ...      0       0    0            0    0       0   0   1    0    0  \n",
       "2  ...      1       0    0            0    0       0   1   1    1    1  \n",
       "\n",
       "[3 rows x 51 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x.toarray(),columns=[features_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307ee2b-8e41-4c6d-af04-dd5d8a6e3fe0",
   "metadata": {},
   "source": [
    "Note :- there is count of word is showing in document matrix . 2 shows word is coming 2 times in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98666c0b-82cf-423b-9c7f-051ca4f6446e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
